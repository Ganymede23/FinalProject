{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET, BEARER_TOKEN\n",
    "import json\n",
    "\n",
    "# V1 AUTH\n",
    "#auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "#auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "#tweepy.Client(bearer_token=BEARER_TOKEN, consumer_key=API_KEY, consumer_secret=API_SECRET, access_token=ACCESS_TOKEN, access_token_secret=ACCESS_TOKEN_SECRET)\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df233bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "\n",
    "def save_list(vehicle_type, url_list):\n",
    "    jsonString = json.dumps(url_list)\n",
    "    path = \"./dataset/\" + vehicle_type + \"/img_urls.json\"\n",
    "    jsonFile = open(path, \"w\")\n",
    "    jsonFile.write(jsonString)\n",
    "    jsonFile.close()\n",
    "\n",
    "def read_list(vehicle_type: str):\n",
    "    path = \"./dataset/\" + vehicle_type + \"/img_urls.json\"\n",
    "    fileObject = open(path, \"r\")\n",
    "    jsonContent = fileObject.read()\n",
    "    try:\n",
    "        url_list = json.loads(jsonContent)\n",
    "    except:\n",
    "        #json is empty\n",
    "        url_list = []\n",
    "        pass\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users of interest: UAWeapons, OSINTua, RALee85, praisethesteph, 200_zoka, oryxspioenkop, Arslon_Xudosi\n",
    "# from:twitterdev\n",
    "# query_t72 = 'T-72 has:images -is:retweet (lang:en OR lang:ru OR lang:uk)'\n",
    "# query_mtlb = '(MT-LB OR MT-LBV OR MT-LBVM OR MT-LBVMK OR MT-LBVM/K) -is:retweet from:UAWeapons'\n",
    "\n",
    "query_vehicle_list = [  # (model,query content)\n",
    "    ('M113', 'M113'),\n",
    "    ('MT-LB','MT-LB OR MT-LBV OR MT-LBVM OR MT-LBVMK OR MT-LBVM/K OR MT-LBu'),\n",
    "    ('BTR-80','BTR-80 OR BTR-82'),\n",
    "    ('BTR-82A','BTR-80A OR BTR-82A'),\n",
    "    ('BMP-1','BMP-1'),\n",
    "    ('BMP-2','BMP-2 OR BMP-2M'),\n",
    "    ('BMP-3','BMP-3 OR BMP-3M'),\n",
    "    ('T-62','T-62 OR T-62M OR T-62MV'),\n",
    "    ('T-64','T-64 OR T-64A OR T-64B OR T-64BV OR T-64B1M OR T-64BM OR T-64BM2'),\n",
    "    ('T-72','T-72 OR T-72A OR T-72AV OR T-72AMT OR T-72B OR T-72BA OR T-72B3 OR T-72B3 OR T-72M OR T-72M1'),\n",
    "    ('T-80','T-80 OR T-80B OR T-80BV OR T-80U OR T-80BVM'),\n",
    "    ('T-90','T-90 OR T-90A OR T-90M'),\n",
    "    ('2S1','2S1 OR Gvozdika'),\n",
    "    ('2S3','2S3 OR Akatsiya'),\n",
    "    ('2S19','2S19 OR 2S19M OR 2S19M1 OR 2S19M2 OR Msta OR Msta-S OR Msta-SM2'),\n",
    "    ('BM-21','BM-21'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_url.json has a list of lists. Each item has this structure: [url, status, media_type, source]\n",
    "\n",
    "for vehicle in query_vehicle_list:\n",
    "    model, query_content = vehicle\n",
    "    \n",
    "    url_counter = 0\n",
    "    url_list = []\n",
    "    url_list = read_list(model)\n",
    "    \n",
    "    # query = '(' + query_content + ') has:images -is:retweet'\n",
    "    query = '(' + query_content + ') (russian OR ukrainian OR russia OR ukraine OR DNR OR DPR OR LNR OR LPR OR captured OR kherson OR kharkiv OR oblast OR donetsk OR severodonetsk OR luhansk OR lugansk OR Dnieper OR Dnipro OR izium OR izyum OR offensive OR attack OR repulsed) has:images -is:retweet'\n",
    "    #print(query)\n",
    "\n",
    "    paginator = tweepy.Paginator(client.search_recent_tweets, query=query, max_results=100, limit=1000, expansions=['attachments.media_keys'], media_fields=['url'])\n",
    "    for page in paginator:\n",
    "        #print(page.includes['media'])  \n",
    "        for item in page.includes['media']:\n",
    "            # [url, status, media_type, source]\n",
    "            if item.url is not None:\n",
    "                if 'pbs.twimg.com/media/' in item.url:\n",
    "                    url_list.append([item.url,'unknown','image','twitter'])\n",
    "                elif 'twitter.com/'  in item.url:\n",
    "                    url_list.append([item.url,'unknown','video','twitter'])\n",
    "                url_counter += 1\n",
    "\n",
    "    save_list(model, url_list)\n",
    "    print('Saved', url_counter, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/70854869/tweepy-problem-retrieving-username-informations-with-twitter-v2-api\n",
    "#https://stackoverflow.com/questions/72016766/tweepy-only-lets-me-get-100-results-how-do-i-get-more-ive-read-about-paginati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function deletes all duplicate URLs by turning List into a Set.\n",
    "from itertools import combinations\n",
    "\n",
    "def remove_url_duplicates(vehicle_type):\n",
    "    # First chunk deletes exact duplicates\n",
    "    url_list = []\n",
    "    url_list = read_list(vehicle_type)\n",
    "\n",
    "    initial_amount = len(url_list)\n",
    "    url_list = [list(item) for item in set(tuple(row) for row in url_list)] # Turns the list of lists into a set to rule out duplicates\n",
    "    # url_list = list(set(url_list))\n",
    "    final_amount = len(url_list)\n",
    "\n",
    "    # Second chunk deletes cases where the URL is the same and keeps the item with status information\n",
    "    # This happens usually when we have a URL retrieved from the Twitter API with status 'unknown', and the same one from Oryx's site\n",
    "    url_only_list = [item[0] for item in url_list]\n",
    "    discarded_urls = []\n",
    "\n",
    "    if len(url_only_list) != len(set(url_only_list)):\n",
    "        for item in combinations(url_list, 2):\n",
    "            item1, item2 = item\n",
    "            url1, status1, _, _ = item1\n",
    "            url2, status2, _, _ = item2\n",
    "            if url1 == url2:\n",
    "                if status1 == 'unknown':\n",
    "                    discarded_urls.append(item1)\n",
    "                else:\n",
    "                    discarded_urls.append(item2)\n",
    "        \n",
    "    url_list = [item for item in url_list if item not in discarded_urls]\n",
    "\n",
    "    save_list(vehicle_type, url_list)\n",
    "    print(vehicle_type, '-', (initial_amount-final_amount)+len(discarded_urls), 'duplicated URLs deleted.')\n",
    "\n",
    "#remove_url_duplicates('M113')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function removes all non-twitter URLs from .json files and applies the new structure to the data. Done to include status, media type and source to each URL.\n",
    "# [url, status, media_type, source]\n",
    "\n",
    "def clear_url_list(vehicle_type):\n",
    "    url_list = []\n",
    "    url_list = read_list(vehicle_type)\n",
    "    new_url_list = []\n",
    "\n",
    "    for item in url_list:\n",
    "        # [url, status, media_type, source]\n",
    "        if isinstance(item, list):\n",
    "            new_url_list.append(item)\n",
    "        else:\n",
    "            if item is not None:\n",
    "                if 'pbs.twimg.com/media/' in item:  # if the media URL belongs to an image\n",
    "                    new_item = [item, 'unknown', 'image', 'twitter']\n",
    "                    new_url_list.append(new_item)\n",
    "                elif 'twitter.com/' in item:        # if the media URL belongs to a video\n",
    "                    new_item = [item, 'unknown', 'video', 'twitter']\n",
    "                    new_url_list.append(new_item)\n",
    "    save_list(vehicle_type, new_url_list)\n",
    "\n",
    "\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    # clear_url_list(vehicle_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73669d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Function gets all of the links from the Oryx's .csv file that correspond to a vehicle of interest\n",
    "def csv_lookup(vehicle_type):\n",
    "    r = requests.get('https://raw.githubusercontent.com/scarnecchia/oryx_data/main/totals_by_system.csv')\n",
    "    oryx_scrape_path = './dataset/oryx_scrape.csv'\n",
    "    with open(oryx_scrape_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    url_list = []\n",
    "    url_list = read_list(vehicle_type)\n",
    "\n",
    "    with open(oryx_scrape_path, newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in csv_reader:\n",
    "            # Can't do list unpacking for some reason, so I have to do it this way\n",
    "            system = row[2]\n",
    "            status = row[3]\n",
    "            url = row[4]\n",
    "\n",
    "            if vehicle_type in system:\n",
    "                counter += 1\n",
    "                # [url, status, media_type, source]\n",
    "                if 'i.postimg.cc/' in url: \n",
    "                    url_list.append([url, status, 'image', 'oryx'])\n",
    "                elif 'https://postimg.cc/' in url: \n",
    "                    url_list.append([url, status, 'website', 'oryx'])\n",
    "                elif 'pbs.twimg.com/media/' in url:\n",
    "                    url_list.append([url, status, 'image', 'twitter'])\n",
    "                elif 'twitter.com/' in url:\n",
    "                    url_list.append([url, status, 'video', 'twitter'])\n",
    "                else:\n",
    "                    url_list.append([url, status, 'unknown', 'unknown'])\n",
    "\n",
    "    save_list(vehicle_type, url_list)\n",
    "    print('Added', counter, vehicle_type, 'URLs to the list. Total number:', len(url_list))\n",
    "\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    csv_lookup(vehicle_type)\n",
    "    remove_url_duplicates(vehicle_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function that counts the amount of .jpg inside a directory. Since the function download_img now assigns names using the very same URL, this function will not be used.\n",
    "def count_img(vehicle_type):\n",
    "    counter = 0\n",
    "    folder_path = './dataset/' + vehicle_type\n",
    "    for path in os.scandir(folder_path):\n",
    "        if path.is_file():\n",
    "            if path.name[-4:] == '.jpg':\n",
    "                counter += 1\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Function downloads images from URLs and assigns them the original name from the same URL. It's done this way in order to prevent downloading the same image twice.\n",
    "def download_img(url, vehicle_type):\n",
    "    if url is not None:\n",
    "        # url received = [url, status, media_type, source]\n",
    "        url, status, media_type, source = url\n",
    "        if media_type == 'video':   \n",
    "            return 'Video'    \n",
    "        elif media_type == 'image':   # if 'https://twitter.com/' not in url:\n",
    "            file_name_temp = url.split('/')\n",
    "            if source == 'twitter':\n",
    "                file_name = file_name_temp[-1]\n",
    "            else: # If source is oryx then we concat the two parts of the URL, since the last one is not unique. \n",
    "                file_name = file_name_temp[-2] + file_name_temp[-1]\n",
    "            full_path = os.path.join('./dataset/',vehicle_type,status,source,file_name) \n",
    "            if not os.path.exists(full_path):\n",
    "                os.makedirs(os.path.dirname(full_path), exist_ok=True) # Creates folder if it doesn't exists previously\n",
    "                r = requests.get(url)  \n",
    "                with open(full_path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return 'Downloaded'\n",
    "            else:\n",
    "                # Picture already exists\n",
    "                return 'Exists'\n",
    "        elif media_type == 'website':\n",
    "            pass\n",
    "        else: # media_type == 'unknown'\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for item in query_vehicle_list:\n",
    "    counter_downloaded = 0\n",
    "    counter_pic_exists = 0\n",
    "    counter_tw_videos = 0\n",
    "\n",
    "    vehicle_type, _ = item\n",
    "    remove_url_duplicates(vehicle_type)\n",
    "    url_list = read_list(vehicle_type)\n",
    "    print('+ Out of', len(url_list), vehicle_type, 'URLs:')\n",
    "    for url in url_list:\n",
    "        outcome = download_img(url, vehicle_type)\n",
    "        if outcome == 'Downloaded': # Downloaded successfully\n",
    "            counter_downloaded += 1\n",
    "        elif outcome == 'Exists':   # Picture already exists\n",
    "            counter_pic_exists += 1\n",
    "        else: # outcome == 'Video': - Link belongs to a Twitter video\n",
    "            counter_tw_videos += 1\n",
    "\n",
    "    print('   -', counter_downloaded, 'new pictures were downloaded')\n",
    "    print('   -', counter_pic_exists, 'pictures already existed')\n",
    "    print('   -', counter_tw_videos, 'URLs belonged to Twitter videos')\n",
    "\n",
    "#download_img('https://twitter.com/UAWeapons/status/1564984095591088129','2S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagehash\n",
    "import os\n",
    "import numpy as np\n",
    "IMAGE_EXTENSIONS = ['.jpg','.jpeg','.bmp','.png', '.gif', '.tiff']\n",
    "\n",
    "def find_duplicates(folder_path,delete_duplicates,verbose=False):\n",
    "        hash_size = 8\n",
    "        \n",
    "        fnames = os.listdir(folder_path)\n",
    "        hashes = {}\n",
    "        duplicates = []\n",
    "\n",
    "        for image in fnames:\n",
    "            if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "                if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                    with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                        temp_hash = imagehash.average_hash(img, hash_size)\n",
    "                        if temp_hash in hashes:\n",
    "                            # print('Duplicate {} \\nfound for Image {}!\\n'.format(image,hashes[temp_hash]))\n",
    "                            if image not in duplicates:\n",
    "                                duplicates.append(image)\n",
    "                        else:\n",
    "                            hashes[temp_hash] = image\n",
    "        \n",
    "        vehicle_name = folder_path.split('/')[-1]\n",
    "        if verbose:\n",
    "            print('\\t',vehicle_name,'- Duplicates:')\n",
    "\n",
    "        if len(duplicates) != 0:\n",
    "            if delete_duplicates:\n",
    "                for img in duplicates:\n",
    "                    os.remove(os.path.join(folder_path,img))\n",
    "                if verbose:\n",
    "                    print('\\t\\t',len(duplicates),'deleted')\n",
    "            else:\n",
    "                if verbose:\n",
    "                    for img in duplicates:\n",
    "                        print('\\t\\t',img)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('\\t\\tNo duplicates found') \n",
    "\n",
    "        return len(duplicates) or 0\n",
    "\n",
    "def find_similar(image_path,folder_path,delete_duplicates,similarity,verbose=False):\n",
    "    hash_size = 8\n",
    "\n",
    "    fnames = os.listdir(folder_path)\n",
    "    threshold = 1 - similarity/100\n",
    "    diff_limit = int(threshold*(hash_size**2))\n",
    "\n",
    "    duplicates = []\n",
    "    image_name = ''\n",
    "    \n",
    "    if os.path.getsize(image_path) > 0:\n",
    "        with Image.open(image_path) as img:\n",
    "            hash1 = imagehash.average_hash(img, hash_size).hash\n",
    "    \n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                    hash2 = imagehash.average_hash(img, hash_size).hash\n",
    "                    if np.count_nonzero(hash1 != hash2) <= diff_limit:\n",
    "                        image_name_temp = image_path.split('/')\n",
    "                        image_name = image_name_temp[-1]\n",
    "                        if image_name != image: # Makes sure the original picture is not added to the list of duplicates\n",
    "                            duplicates.append(image)\n",
    "                            # print('{} image found {}% similar to {}'.format(image,similarity,folder_path))\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\tOriginal:')\n",
    "        print('\\t\\t',image_name)\n",
    "        print('\\tDuplicates:')\n",
    "\n",
    "    if len(duplicates) != 0:\n",
    "        if delete_duplicates:\n",
    "            for img in duplicates:\n",
    "                os.remove(os.path.join(folder_path,img))\n",
    "            if verbose:\n",
    "                print('\\t\\t',len(duplicates))\n",
    "        else:\n",
    "            if verbose:\n",
    "                for img in duplicates:\n",
    "                    print('\\t\\t',img)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('\\t\\tNo close duplicates found') \n",
    "    \n",
    "    return len(duplicates) or 0\n",
    "    \n",
    "#find_similar('./dd/2S1 - Copy/R0r6NcvQ5577.png','./dd/2S1 - Copy/',False,97,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_duplicates('./dd/orig1/',True)\n",
    "\n",
    "#for item in query_vehicle_list:\n",
    "\n",
    "folder_path = './dd/c3/'\n",
    "fnames = os.listdir(folder_path)\n",
    "for image in fnames:\n",
    "    image_path = os.path.join(folder_path,image)\n",
    "    if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "        find_similar(image_path,folder_path,True,97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    folder_path = './dataset/' + vehicle_type\n",
    "    amount_duplicates = find_duplicates(folder_path, True)\n",
    "    \n",
    "    total_close_duplicates = 0\n",
    "    folder_path += '/'\n",
    "    fnames = os.listdir(folder_path)\n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            image_path = os.path.join(folder_path,image)\n",
    "            if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "                amount_close_duplicates = find_similar(image_path,folder_path,True,97)\n",
    "                total_close_duplicates += amount_close_duplicates\n",
    "    print(vehicle_type, '-', amount_duplicates, 'duplicates and', total_close_duplicates, 'close duplicates deleted.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ae438818e454af4f26ac53bc90cc6114af98306db0d7e90db8f6b9a9700471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
