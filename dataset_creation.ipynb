{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET, BEARER_TOKEN\n",
    "import json\n",
    "\n",
    "# V1 AUTH\n",
    "#auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "#auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "#tweepy.Client(bearer_token=BEARER_TOKEN, consumer_key=API_KEY, consumer_secret=API_SECRET, access_token=ACCESS_TOKEN, access_token_secret=ACCESS_TOKEN_SECRET)\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df233bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "\n",
    "def save_list(vehicle_type, url_list):\n",
    "    jsonString = json.dumps(url_list)\n",
    "    path = \"./dataset/\" + vehicle_type + \"/img_urls.json\"\n",
    "    jsonFile = open(path, \"w\")\n",
    "    jsonFile.write(jsonString)\n",
    "    jsonFile.close()\n",
    "    \n",
    "\n",
    "def read_list(vehicle_type: str):\n",
    "    path = \"./dataset/\" + vehicle_type + \"/img_urls.json\"\n",
    "    fileObject = open(path, \"r\")\n",
    "    jsonContent = fileObject.read()\n",
    "    try:\n",
    "        url_list = json.loads(jsonContent)\n",
    "    except:\n",
    "        #json is empty\n",
    "        url_list = []\n",
    "        pass\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f51a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users of interest: UAWeapons, OSINTua, RALee85, praisethesteph, 200_zoka, oryxspioenkop, Arslon_Xudosi\n",
    "# from:twitterdev\n",
    "# query_t72 = 'T-72 has:images -is:retweet (lang:en OR lang:ru OR lang:uk)'\n",
    "# query_mtlb = '(MT-LB OR MT-LBV OR MT-LBVM OR MT-LBVMK OR MT-LBVM/K) -is:retweet from:UAWeapons'\n",
    "\n",
    "query_vehicle_list = [  # (model,query content)\n",
    "    ('MT-LB','MT-LB OR MT-LBV OR MT-LBVM OR MT-LBVMK OR MT-LBVM/K OR MT-LBu'),\n",
    "    ('BTR-80','BTR-80 OR BTR-82'),\n",
    "    ('BTR-82A','BTR-80A OR BTR-82A'),\n",
    "    ('BMP-1','BMP-1'),\n",
    "    ('BMP-2','BMP-2 OR BMP-2M'),\n",
    "    ('BMP-3','BMP-3 OR BMP-3M'),\n",
    "    ('T-62','T-62 OR T-62M OR T-62MV'),\n",
    "    ('T-64','T-64 OR T-64A OR T-64B OR T-64BV OR T-64B1M OR T-64BM OR T-64BM2'),\n",
    "    ('T-72','T-72 OR T-72A OR T-72AV OR T-72AMT OR T-72B OR T-72BA OR T-72B3 OR T-72B3 OR T-72M OR T-72M1'),\n",
    "    ('T-80','T-80 OR T-80B OR T-80BV OR T-80U OR T-80BVM'),\n",
    "    ('T-90','T-90 OR T-90A OR T-90M'),\n",
    "    ('2S1','2S1 OR Gvozdika'),\n",
    "    ('2S3','2S3 OR Akatsiya'),\n",
    "    ('2S19','2S19 OR 2S19M OR 2S19M1 OR 2S19M2 OR Msta OR Msta-S OR Msta-SM2'),\n",
    "    ('BM-21','BM-21'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vehicle in query_vehicle_list:\n",
    "    model, query_content = vehicle\n",
    "    \n",
    "    url_counter = 0\n",
    "    url_list = []\n",
    "    url_list = read_list(model)\n",
    "    \n",
    "    # query = '(' + query_content + ') has:images -is:retweet'\n",
    "    query = '(' + query_content + ') (russian OR ukrainian OR ukraine OR russia OR army OR captured OR kherson OR kharkiv OR oblast OR donetsk OR severodonetsk OR luhansk OR lugansk OR Dnieper OR Dnipro OR izium OR izyum) has:images -is:retweet'\n",
    "    #print(query)\n",
    "\n",
    "    paginator = tweepy.Paginator(client.search_recent_tweets, query=query, max_results=100, limit=1000, expansions=['attachments.media_keys'], media_fields=['url'])\n",
    "    for page in paginator:\n",
    "        #print(page.includes['media'])  \n",
    "        for item in page.includes['media']:\n",
    "            url_list.append(item.url)\n",
    "            url_counter += 1\n",
    "\n",
    "    save_list(model, url_list)\n",
    "    print('Saved', url_counter, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/70854869/tweepy-problem-retrieving-username-informations-with-twitter-v2-api\n",
    "#https://stackoverflow.com/questions/72016766/tweepy-only-lets-me-get-100-results-how-do-i-get-more-ive-read-about-paginati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function deletes all duplicate URLs by turning List into a Set.\n",
    "def remove_url_duplicates(vehicle_type):\n",
    "    url_list = []\n",
    "    url_list = read_list(vehicle_type)\n",
    "    \n",
    "    initial_amount = len(url_list)\n",
    "    url_list = list(set(url_list))\n",
    "    final_amount = len(url_list)\n",
    "\n",
    "    save_list(vehicle_type, url_list)\n",
    "    print(vehicle_type, '-', initial_amount - final_amount, 'duplicated URLs deleted.')\n",
    "\n",
    "#remove_url_duplicates('2S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73669d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function gets all of the links from the Oryx's .csv file that correspond to a vehicle of interest\n",
    "def csv_lookup(vehicle_type):\n",
    "    counter = 0\n",
    "\n",
    "    url_list = []\n",
    "    url_list = read_list(vehicle_type)\n",
    "\n",
    "    with open('./dataset/totals_by_system.csv', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in csv_reader:\n",
    "            #Can't do list unpacking for some reason, so I have to do it this way\n",
    "            system = row[2]\n",
    "            status = row[3]\n",
    "            url = row[4]\n",
    "\n",
    "            if vehicle_type in system:\n",
    "                counter += 1\n",
    "                url_list.append(url)\n",
    "\n",
    "    save_list(vehicle_type, url_list)\n",
    "    print('Added', counter, vehicle_type, 'URLs to the list. Total number:', len(url_list))\n",
    "\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    csv_lookup(vehicle_type)\n",
    "    remove_url_duplicates(vehicle_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function that counts the amount of .jpg inside a directory. Since the function download_img now assigns names using the very same URL, this function will not be used.\n",
    "def count_img(vehicle_type):\n",
    "    counter = 0\n",
    "    folder_path = './dataset/' + vehicle_type\n",
    "    for path in os.scandir(folder_path):\n",
    "        if path.is_file():\n",
    "            if path.name[-4:] == '.jpg':\n",
    "                counter += 1\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib.request\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Function downloads images from URLs and assigns them the original name from the same URL. It's done this way in order to prevent downloading the same image twice.\n",
    "def download_img(url, vehicle_type):\n",
    "    if url is not None:\n",
    "        if 'https://twitter.com/' not in url:\n",
    "            file_name_temp = url.split('/')\n",
    "            if 'pbs.twimg.com/media/' in url: # In case the URL is a regular Twitter image\n",
    "                file_name = file_name_temp[-1]\n",
    "            else: # In case the URL is something like 'i.postimg.cc/NFwX6m7d/763.png'\n",
    "                file_name = file_name_temp[-2] + file_name_temp[-1]\n",
    "            full_path = './dataset/' + vehicle_type + '/' + file_name\n",
    "            if not os.path.exists(full_path):\n",
    "                #urllib.request.urlretrieve(url, full_path)\n",
    "                r = requests.get(url)  \n",
    "                with open(full_path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return 'Downloaded'\n",
    "            else:\n",
    "                # Picture already exists\n",
    "                return 'Exists'\n",
    "        else:\n",
    "            # URL belongs to a Twitter video\n",
    "            return 'Video'\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for item in query_vehicle_list:\n",
    "    counter_downloaded = 0\n",
    "    counter_pic_exists = 0\n",
    "    counter_tw_videos = 0\n",
    "\n",
    "    vehicle_type, _ = item\n",
    "    remove_url_duplicates(vehicle_type)\n",
    "    url_list = read_list(vehicle_type)\n",
    "    print('+ Out of', len(url_list), vehicle_type, 'URLs:')\n",
    "    for url in url_list:\n",
    "        outcome = download_img(url, vehicle_type)\n",
    "        if outcome == 'Downloaded': # Downloaded successfully\n",
    "            counter_downloaded += 1\n",
    "        elif outcome == 'Exists':   # Picture already exists\n",
    "            counter_pic_exists += 1\n",
    "        else: # outcome == 'Video': - Link belongs to a Twitter video\n",
    "            counter_tw_videos += 1\n",
    "\n",
    "    print('   -', counter_downloaded, 'new pictures were downloaded')\n",
    "    print('   -', counter_pic_exists, 'pictures already existed')\n",
    "    print('   -', counter_tw_videos, 'URLs belonged to Twitter videos')\n",
    "\n",
    "#download_img('https://twitter.com/UAWeapons/status/1564984095591088129','2S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagehash\n",
    "import os\n",
    "import numpy as np\n",
    "IMAGE_EXTENSIONS = ['.jpg','.jpeg','.bmp','.png', '.gif', '.tiff']\n",
    "\n",
    "def find_duplicates(folder_path,delete_duplicates,verbose=False):\n",
    "        hash_size = 8\n",
    "        \n",
    "        fnames = os.listdir(folder_path)\n",
    "        hashes = {}\n",
    "        duplicates = []\n",
    "\n",
    "        for image in fnames:\n",
    "            if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "                if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                    with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                        temp_hash = imagehash.average_hash(img, hash_size)\n",
    "                        if temp_hash in hashes:\n",
    "                            # print('Duplicate {} \\nfound for Image {}!\\n'.format(image,hashes[temp_hash]))\n",
    "                            if image not in duplicates:\n",
    "                                duplicates.append(image)\n",
    "                        else:\n",
    "                            hashes[temp_hash] = image\n",
    "        \n",
    "        vehicle_name = folder_path.split('/')[-1]\n",
    "        if verbose:\n",
    "            print('\\t',vehicle_name,'- Duplicates:')\n",
    "\n",
    "        if len(duplicates) != 0:\n",
    "            if delete_duplicates:\n",
    "                for img in duplicates:\n",
    "                    os.remove(os.path.join(folder_path,img))\n",
    "                if verbose:\n",
    "                    print('\\t\\t',len(duplicates),'deleted')\n",
    "            else:\n",
    "                if verbose:\n",
    "                    for img in duplicates:\n",
    "                        print('\\t\\t',img)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('\\t\\tNo duplicates found') \n",
    "\n",
    "        return len(duplicates) or 0\n",
    "\n",
    "def find_similar(image_path,folder_path,delete_duplicates,similarity,verbose=False):\n",
    "    hash_size = 8\n",
    "\n",
    "    fnames = os.listdir(folder_path)\n",
    "    threshold = 1 - similarity/100\n",
    "    diff_limit = int(threshold*(hash_size**2))\n",
    "\n",
    "    duplicates = []\n",
    "    image_name = ''\n",
    "    \n",
    "    if os.path.getsize(image_path) > 0:\n",
    "        with Image.open(image_path) as img:\n",
    "            hash1 = imagehash.average_hash(img, hash_size).hash\n",
    "    \n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                    hash2 = imagehash.average_hash(img, hash_size).hash\n",
    "                    if np.count_nonzero(hash1 != hash2) <= diff_limit:\n",
    "                        image_name_temp = image_path.split('/')\n",
    "                        image_name = image_name_temp[-1]\n",
    "                        if image_name != image: # Makes sure the original picture is not added to the list of duplicates\n",
    "                            duplicates.append(image)\n",
    "                            # print('{} image found {}% similar to {}'.format(image,similarity,folder_path))\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\tOriginal:')\n",
    "        print('\\t\\t',image_name)\n",
    "        print('\\tDuplicates:')\n",
    "\n",
    "    if len(duplicates) != 0:\n",
    "        if delete_duplicates:\n",
    "            for img in duplicates:\n",
    "                os.remove(os.path.join(folder_path,img))\n",
    "            if verbose:\n",
    "                print('\\t\\t',len(duplicates))\n",
    "        else:\n",
    "            if verbose:\n",
    "                for img in duplicates:\n",
    "                    print('\\t\\t',img)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('\\t\\tNo close duplicates found') \n",
    "    \n",
    "    return len(duplicates) or 0\n",
    "    \n",
    "#find_similar('./dd/2S1 - Copy/R0r6NcvQ5577.png','./dd/2S1 - Copy/',False,97,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_duplicates('./dd/orig1/',True)\n",
    "\n",
    "#for item in query_vehicle_list:\n",
    "\n",
    "folder_path = './dd/c3/'\n",
    "fnames = os.listdir(folder_path)\n",
    "for image in fnames:\n",
    "    image_path = os.path.join(folder_path,image)\n",
    "    if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "        find_similar(image_path,folder_path,True,97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    folder_path = './dataset/' + vehicle_type\n",
    "    amount_duplicates = find_duplicates(folder_path, True)\n",
    "    \n",
    "    total_close_duplicates = 0\n",
    "    folder_path += '/'\n",
    "    fnames = os.listdir(folder_path)\n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            image_path = os.path.join(folder_path,image)\n",
    "            if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "                amount_close_duplicates = find_similar(image_path,folder_path,True,97)\n",
    "                total_close_duplicates += amount_close_duplicates\n",
    "    print(vehicle_type, '-', amount_duplicates, 'duplicates and', total_close_duplicates, 'close duplicates deleted.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ae438818e454af4f26ac53bc90cc6114af98306db0d7e90db8f6b9a9700471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
