{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_functions import read_list, save_list, count_urls, download_media\n",
    "from data_cleansing_functions import remove_url_duplicates, remove_all_but_twitter_urls\n",
    "from web_scraping_functions import query_vehicle_list, twitter_scrape, oryx_scrape, warspotting_scrape\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter stuff\n",
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET, BEARER_TOKEN\n",
    "\n",
    "# Twitter Auth\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3094d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Scrape\n",
    "counter = 0\n",
    "print('Scraping Twitter:')\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    new_item_amount, total_amount = twitter_scrape(vehicle_type)\n",
    "    # print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "    # remove_url_duplicates(vehicle_type)\n",
    "    counter += new_item_amount\n",
    "\n",
    "print(f'\\n\\tTotal amount of URLs added: {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oryx Scrape\n",
    "print('Scraping Oryx:')\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    new_item_amount, total_amount = oryx_scrape(vehicle_type)\n",
    "    # print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "    # remove_url_duplicates(vehicle_type)\n",
    "\n",
    "# oryx_scrape('T-62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warspotting Scrape (async)\n",
    "import concurrent.futures\n",
    "processes = []\n",
    "\n",
    "query_vehicle_list = [['T-90',1],['M113',2],['2S1',1],['2S3',2]] # Custom list for testing purposes\n",
    "\n",
    "print('Scraping Warspotting:')\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    if __name__ == \"__main__\":\n",
    "        for item in query_vehicle_list:\n",
    "            vehicle_type, _ = item\n",
    "            p = executor.submit(warspotting_scrape,vehicle_type)\n",
    "            processes.append([p,vehicle_type])\n",
    "        for p in processes:\n",
    "            process, vehicle_type = p\n",
    "            new_item_amount, total_amount = process.result()\n",
    "            print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "\n",
    "# Warspotting Scrape (sync)\n",
    "# warspotting_scrape('T-62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc19c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    #remove_url_duplicates(vehicle_type)\n",
    "    count_urls(vehicle_type)\n",
    "    #remove_all_but_twitter_urls(vehicle_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75333e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# url_list = read_list('T-90')\n",
    "# df = pd.DataFrame(url_list, columns = ['URL', 'Status', 'Media Type', 'Source'])\n",
    "# print(df['Source'].value_counts())\n",
    "\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    url_list = read_list(vehicle_type)\n",
    "    df = pd.DataFrame(url_list, columns = ['URL', 'Status', 'Media Type', 'Source'])\n",
    "    print()\n",
    "    print(vehicle_type)\n",
    "    print(df['Source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b22f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_responses, failed_img_requests = download_media('2S1')\n",
    "query_vehicle_list = [['XX1',1], ['XX2',1]]\n",
    "\n",
    "print('Downloading media:')\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    img_responses, failed_img_requests = download_media(vehicle_type)\n",
    "    # print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "    # remove_url_duplicates(vehicle_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagehash\n",
    "import os\n",
    "import numpy as np\n",
    "IMAGE_EXTENSIONS = ['.jpg','.jpeg','.bmp','.png', '.gif', '.tiff']\n",
    "\n",
    "def find_duplicates(folder_path,delete_duplicates,verbose=False):\n",
    "        hash_size = 8\n",
    "        \n",
    "        fnames = os.listdir(folder_path)\n",
    "        hashes = {}\n",
    "        duplicates = []\n",
    "\n",
    "        for image in fnames:\n",
    "            if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "                if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                    with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                        temp_hash = imagehash.average_hash(img, hash_size)\n",
    "                        if temp_hash in hashes:\n",
    "                            # print('Duplicate {} \\nfound for Image {}!\\n'.format(image,hashes[temp_hash]))\n",
    "                            if image not in duplicates:\n",
    "                                duplicates.append(image)\n",
    "                        else:\n",
    "                            hashes[temp_hash] = image\n",
    "        \n",
    "        vehicle_name = folder_path.split('/')[-1]\n",
    "        if verbose:\n",
    "            print('\\t',vehicle_name,'- Duplicates:')\n",
    "\n",
    "        if len(duplicates) != 0:\n",
    "            if delete_duplicates:\n",
    "                for img in duplicates:\n",
    "                    os.remove(os.path.join(folder_path,img))\n",
    "                if verbose:\n",
    "                    print('\\t\\t',len(duplicates),'deleted')\n",
    "            else:\n",
    "                if verbose:\n",
    "                    for img in duplicates:\n",
    "                        print('\\t\\t',img)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('\\t\\tNo duplicates found') \n",
    "\n",
    "        return len(duplicates) or 0\n",
    "\n",
    "def find_similar(image_path,folder_path,delete_duplicates,similarity,verbose=False):\n",
    "    hash_size = 8\n",
    "\n",
    "    fnames = os.listdir(folder_path)\n",
    "    threshold = 1 - similarity/100\n",
    "    diff_limit = int(threshold*(hash_size**2))\n",
    "\n",
    "    duplicates = []\n",
    "    image_name = ''\n",
    "    \n",
    "    if os.path.getsize(image_path) > 0:\n",
    "        with Image.open(image_path) as img:\n",
    "            hash1 = imagehash.average_hash(img, hash_size).hash\n",
    "    \n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                    hash2 = imagehash.average_hash(img, hash_size).hash\n",
    "                    if np.count_nonzero(hash1 != hash2) <= diff_limit:\n",
    "                        image_name_temp = image_path.split('/')\n",
    "                        image_name = image_name_temp[-1]\n",
    "                        if image_name != image: # Makes sure the original picture is not added to the list of duplicates\n",
    "                            duplicates.append(image)\n",
    "                            # print('{} image found {}% similar to {}'.format(image,similarity,folder_path))\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\tOriginal:')\n",
    "        print('\\t\\t',image_name)\n",
    "        print('\\tDuplicates:')\n",
    "\n",
    "    if len(duplicates) != 0:\n",
    "        if delete_duplicates:\n",
    "            for img in duplicates:\n",
    "                os.remove(os.path.join(folder_path,img))\n",
    "            if verbose:\n",
    "                print('\\t\\t',len(duplicates))\n",
    "        else:\n",
    "            if verbose:\n",
    "                for img in duplicates:\n",
    "                    print('\\t\\t',img)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('\\t\\tNo close duplicates found') \n",
    "    \n",
    "    return len(duplicates) or 0\n",
    "    \n",
    "#find_similar('./dd/2S1 - Copy/R0r6NcvQ5577.png','./dd/2S1 - Copy/',False,97,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_duplicates('./dd/orig1/',True)\n",
    "\n",
    "#for item in query_vehicle_list:\n",
    "\n",
    "folder_path = './dd/c3/'\n",
    "fnames = os.listdir(folder_path)\n",
    "for image in fnames:\n",
    "    image_path = os.path.join(folder_path,image)\n",
    "    if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "        find_similar(image_path,folder_path,True,97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    folder_path = './dataset/' + vehicle_type\n",
    "    amount_duplicates = find_duplicates(folder_path, True)\n",
    "    \n",
    "    total_close_duplicates = 0\n",
    "    folder_path += '/'\n",
    "    fnames = os.listdir(folder_path)\n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            image_path = os.path.join(folder_path,image)\n",
    "            if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "                amount_close_duplicates = find_similar(image_path,folder_path,True,97)\n",
    "                total_close_duplicates += amount_close_duplicates\n",
    "    print(vehicle_type, '-', amount_duplicates, 'duplicates and', total_close_duplicates, 'close duplicates deleted.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ae438818e454af4f26ac53bc90cc6114af98306db0d7e90db8f6b9a9700471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
