{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from twitter_authentication import API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET, BEARER_TOKEN\n",
    "from file_functions import read_list, save_list, count_urls\n",
    "from data_cleansing_functions import remove_url_duplicates, remove_all_but_twitter_urls\n",
    "from web_scraping_functions import oryx_scrape, warspotting_scrape\n",
    "\n",
    "# V1 AUTH\n",
    "#auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "#auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "#tweepy.Client(bearer_token=BEARER_TOKEN, consumer_key=API_KEY, consumer_secret=API_SECRET, access_token=ACCESS_TOKEN, access_token_secret=ACCESS_TOKEN_SECRET)\n",
    "\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1919c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_url.json has a list of lists. Each item has this structure: [url, status, media_type, source]\n",
    "\n",
    "# Users of interest: UAWeapons, OSINTua, RALee85, praisethesteph, 200_zoka, oryxspioenkop, Arslon_Xudosi\n",
    "# from:twitterdev\n",
    "# query_t72 = 'T-72 has:images -is:retweet (lang:en OR lang:ru OR lang:uk)'\n",
    "# query_mtlb = '(MT-LB OR MT-LBV OR MT-LBVM OR MT-LBVMK OR MT-LBVM/K) -is:retweet from:UAWeapons'\n",
    "\n",
    "query_vehicle_list = [  # (model,query content)\n",
    "    ('M113', 'M113'),\n",
    "    ('MT-LB','MT-LB OR MT-LBV OR MT-LBVM OR MT-LBVMK OR MT-LBVM/K OR MT-LBu'),\n",
    "    ('BTR-80','BTR-80 OR BTR-82'),\n",
    "    ('BTR-82A','BTR-80A OR BTR-82A'),\n",
    "    ('BMP-1','BMP-1'),\n",
    "    ('BMP-2','BMP-2 OR BMP-2M'),\n",
    "    ('BMP-3','BMP-3 OR BMP-3M'),\n",
    "    ('T-62','T-62 OR T-62M OR T-62MV'),\n",
    "    ('T-64','T-64 OR T-64A OR T-64B OR T-64BV OR T-64B1M OR T-64BM OR T-64BM2'),\n",
    "    ('T-72','T-72 OR T-72A OR T-72AV OR T-72AMT OR T-72B OR T-72BA OR T-72B3 OR T-72B3 OR T-72M OR T-72M1'),\n",
    "    ('T-80','T-80 OR T-80B OR T-80BV OR T-80U OR T-80BVM'),\n",
    "    ('T-90','T-90 OR T-90A OR T-90M'),\n",
    "    ('2S1','2S1 OR Gvozdika'),\n",
    "    ('2S3','2S3 OR Akatsiya'),\n",
    "    ('2S19','2S19 OR 2S19M OR 2S19M1 OR 2S19M2 OR Msta OR Msta-S OR Msta-SM2'),\n",
    "    ('BM-21','BM-21'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4371428",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vehicle in query_vehicle_list:\n",
    "    model, query_content = vehicle\n",
    "    \n",
    "    url_counter = 0\n",
    "    url_list = []\n",
    "    url_list = read_list(model)\n",
    "    \n",
    "    # query = '(' + query_content + ') has:images -is:retweet'\n",
    "    query = '(' + query_content + ') (russian OR ukrainian OR russia OR ukraine OR DNR OR DPR OR LNR OR LPR OR captured OR abandoned OR damaged OR destroyed OR kherson OR kharkiv OR oblast OR donetsk OR severodonetsk OR luhansk OR lugansk OR Dnieper OR Dnipro OR izium OR izyum OR offensive OR attack OR repulsed) has:images -is:retweet'\n",
    "    #print(query)\n",
    "\n",
    "    paginator = tweepy.Paginator(client.search_recent_tweets, query=query, max_results=100, limit=1000, expansions=['attachments.media_keys'], media_fields=['url'])\n",
    "    for page in paginator:\n",
    "        #print(page.includes['media'])  \n",
    "        for item in page.includes['media']:\n",
    "            # [url, status, media_type, source]\n",
    "            if item.url is not None:\n",
    "                if 'pbs.twimg.com/media/' in item.url:\n",
    "                    url_list.append([item.url,'unknown','image','twitter'])\n",
    "                elif 'twitter.com/'  in item.url:\n",
    "                    url_list.append([item.url,'unknown','video','twitter'])\n",
    "                url_counter += 1\n",
    "\n",
    "    save_list(model, url_list)\n",
    "    print('Saved', url_counter, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/70854869/tweepy-problem-retrieving-username-informations-with-twitter-v2-api\n",
    "#https://stackoverflow.com/questions/72016766/tweepy-only-lets-me-get-100-results-how-do-i-get-more-ive-read-about-paginati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warspotting Scrape (async)\n",
    "import concurrent.futures\n",
    "processes = []\n",
    "\n",
    "# query_vehicle_list = [['T-90',1],['M113',2],['2S1',1],['2S3',2]] # Custom list for testin purposes\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    if __name__ == \"__main__\":\n",
    "        for item in query_vehicle_list:\n",
    "            vehicle_type, _ = item\n",
    "            p = executor.submit(warspotting_scrape,vehicle_type)\n",
    "            processes.append([p,vehicle_type])\n",
    "        for p in processes:\n",
    "            process, vehicle_type = p\n",
    "            new_item_amount, total_amount = process.result()\n",
    "            print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "\n",
    "# Warspotting Scrape (sync)\n",
    "# warspotting_scrape('T-62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oryx Scrape\n",
    "new_item_amount = 0\n",
    "total_amount = 0\n",
    "\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    new_item_amount, total_amount = oryx_scrape(vehicle_type)\n",
    "    print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "    # remove_url_duplicates(vehicle_type)\n",
    "\n",
    "# oryx_scrape('T-62')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc19c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    remove_url_duplicates(vehicle_type)\n",
    "    #count_urls(vehicle_type, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75333e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url_list = read_list('T-90')\n",
    "df = pd.DataFrame(url_list, columns = ['URL', 'Status', 'Media Type', 'Source'])\n",
    "print(df['Source'].value_counts())\n",
    "print('===')\n",
    "print(df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Function downloads images from URLs and assigns them the original name from the same URL. It's done this way in order to prevent downloading the same image twice.\n",
    "def download_img(url, vehicle_type):\n",
    "    if url is not None:\n",
    "        # url received = [url, status, media_type, source]\n",
    "        url, status, media_type, source = url\n",
    "        if media_type == 'video':   \n",
    "            return 'Video'    \n",
    "        elif media_type == 'image':   # if 'https://twitter.com/' not in url:\n",
    "            file_name_temp = url.split('/')\n",
    "            if source == 'twitter':\n",
    "                file_name = file_name_temp[-1]\n",
    "            else: # If source is oryx then we concat the two parts of the URL, since the last one is not unique. \n",
    "                file_name = file_name_temp[-2] + file_name_temp[-1]\n",
    "            full_path = os.path.join('./dataset/',vehicle_type,status,source,file_name) \n",
    "            if not os.path.exists(full_path):\n",
    "                os.makedirs(os.path.dirname(full_path), exist_ok=True) # Creates folder if it doesn't exists previously\n",
    "                r = requests.get(url)  \n",
    "                with open(full_path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return 'Downloaded'\n",
    "            else:\n",
    "                # Picture already exists\n",
    "                return 'Exists'\n",
    "        elif media_type == 'website':\n",
    "            pass\n",
    "        else: # media_type == 'unknown'\n",
    "            pass\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vehicle_list2 = [\n",
    "    ['M113',1],\n",
    "    ['T-64',2]\n",
    "]\n",
    "for item in query_vehicle_list2:\n",
    "    counter_downloaded = 0\n",
    "    counter_pic_exists = 0\n",
    "    counter_tw_videos = 0\n",
    "\n",
    "    vehicle_type, _ = item\n",
    "    remove_url_duplicates(vehicle_type)\n",
    "    url_list = read_list(vehicle_type)\n",
    "    print('+ Out of', len(url_list), vehicle_type, 'URLs:')\n",
    "    for url in url_list:\n",
    "        outcome = download_img(url, vehicle_type)\n",
    "        if outcome == 'Downloaded': # Downloaded successfully\n",
    "            counter_downloaded += 1\n",
    "        elif outcome == 'Exists':   # Picture already exists\n",
    "            counter_pic_exists += 1\n",
    "        else: # outcome == 'Video': - Link belongs to a Twitter video\n",
    "            counter_tw_videos += 1\n",
    "\n",
    "    print('   -', counter_downloaded, 'new pictures were downloaded')\n",
    "    print('   -', counter_pic_exists, 'pictures already existed')\n",
    "    print('   -', counter_tw_videos, 'URLs belonged to Twitter videos')\n",
    "\n",
    "#download_img('https://twitter.com/UAWeapons/status/1564984095591088129','2S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_item_amount_1 = 0\n",
    "total_amount_1 = 0\n",
    "\n",
    "new_item_amount_1, total_amount_1 = warspotting_scrape('T-90')\n",
    "new_item_amount_1, total_amount_1 = warspotting_scrape('2S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65580c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import concurrent.futures\n",
    "from web_scraping_functions import warspotting_scrape\n",
    "import time\n",
    "\n",
    "p1 = Process(target=warspotting_scrape, args=['T-90'])\n",
    "p2 = Process(target=warspotting_scrape, args=['2S1'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "finish = time.perf_counter()\n",
    "print('It took: ', finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagehash\n",
    "import os\n",
    "import numpy as np\n",
    "IMAGE_EXTENSIONS = ['.jpg','.jpeg','.bmp','.png', '.gif', '.tiff']\n",
    "\n",
    "def find_duplicates(folder_path,delete_duplicates,verbose=False):\n",
    "        hash_size = 8\n",
    "        \n",
    "        fnames = os.listdir(folder_path)\n",
    "        hashes = {}\n",
    "        duplicates = []\n",
    "\n",
    "        for image in fnames:\n",
    "            if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "                if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                    with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                        temp_hash = imagehash.average_hash(img, hash_size)\n",
    "                        if temp_hash in hashes:\n",
    "                            # print('Duplicate {} \\nfound for Image {}!\\n'.format(image,hashes[temp_hash]))\n",
    "                            if image not in duplicates:\n",
    "                                duplicates.append(image)\n",
    "                        else:\n",
    "                            hashes[temp_hash] = image\n",
    "        \n",
    "        vehicle_name = folder_path.split('/')[-1]\n",
    "        if verbose:\n",
    "            print('\\t',vehicle_name,'- Duplicates:')\n",
    "\n",
    "        if len(duplicates) != 0:\n",
    "            if delete_duplicates:\n",
    "                for img in duplicates:\n",
    "                    os.remove(os.path.join(folder_path,img))\n",
    "                if verbose:\n",
    "                    print('\\t\\t',len(duplicates),'deleted')\n",
    "            else:\n",
    "                if verbose:\n",
    "                    for img in duplicates:\n",
    "                        print('\\t\\t',img)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('\\t\\tNo duplicates found') \n",
    "\n",
    "        return len(duplicates) or 0\n",
    "\n",
    "def find_similar(image_path,folder_path,delete_duplicates,similarity,verbose=False):\n",
    "    hash_size = 8\n",
    "\n",
    "    fnames = os.listdir(folder_path)\n",
    "    threshold = 1 - similarity/100\n",
    "    diff_limit = int(threshold*(hash_size**2))\n",
    "\n",
    "    duplicates = []\n",
    "    image_name = ''\n",
    "    \n",
    "    if os.path.getsize(image_path) > 0:\n",
    "        with Image.open(image_path) as img:\n",
    "            hash1 = imagehash.average_hash(img, hash_size).hash\n",
    "    \n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                    hash2 = imagehash.average_hash(img, hash_size).hash\n",
    "                    if np.count_nonzero(hash1 != hash2) <= diff_limit:\n",
    "                        image_name_temp = image_path.split('/')\n",
    "                        image_name = image_name_temp[-1]\n",
    "                        if image_name != image: # Makes sure the original picture is not added to the list of duplicates\n",
    "                            duplicates.append(image)\n",
    "                            # print('{} image found {}% similar to {}'.format(image,similarity,folder_path))\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\tOriginal:')\n",
    "        print('\\t\\t',image_name)\n",
    "        print('\\tDuplicates:')\n",
    "\n",
    "    if len(duplicates) != 0:\n",
    "        if delete_duplicates:\n",
    "            for img in duplicates:\n",
    "                os.remove(os.path.join(folder_path,img))\n",
    "            if verbose:\n",
    "                print('\\t\\t',len(duplicates))\n",
    "        else:\n",
    "            if verbose:\n",
    "                for img in duplicates:\n",
    "                    print('\\t\\t',img)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('\\t\\tNo close duplicates found') \n",
    "    \n",
    "    return len(duplicates) or 0\n",
    "    \n",
    "#find_similar('./dd/2S1 - Copy/R0r6NcvQ5577.png','./dd/2S1 - Copy/',False,97,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_duplicates('./dd/orig1/',True)\n",
    "\n",
    "#for item in query_vehicle_list:\n",
    "\n",
    "folder_path = './dd/c3/'\n",
    "fnames = os.listdir(folder_path)\n",
    "for image in fnames:\n",
    "    image_path = os.path.join(folder_path,image)\n",
    "    if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "        find_similar(image_path,folder_path,True,97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    folder_path = './dataset/' + vehicle_type\n",
    "    amount_duplicates = find_duplicates(folder_path, True)\n",
    "    \n",
    "    total_close_duplicates = 0\n",
    "    folder_path += '/'\n",
    "    fnames = os.listdir(folder_path)\n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            image_path = os.path.join(folder_path,image)\n",
    "            if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "                amount_close_duplicates = find_similar(image_path,folder_path,True,97)\n",
    "                total_close_duplicates += amount_close_duplicates\n",
    "    print(vehicle_type, '-', amount_duplicates, 'duplicates and', total_close_duplicates, 'close duplicates deleted.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ae438818e454af4f26ac53bc90cc6114af98306db0d7e90db8f6b9a9700471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
