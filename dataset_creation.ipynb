{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from file_functions import read_list, save_list, count_urls\n",
    "from data_cleansing_functions import remove_url_duplicates, remove_all_but_twitter_urls\n",
    "from web_scraping_functions import query_vehicle_list, twitter_scrape, oryx_scrape, warspotting_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3094d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Scrape\n",
    "counter = 0\n",
    "print('Scraping Twitter:')\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    new_item_amount, total_amount = twitter_scrape(vehicle_type)\n",
    "    # print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "    # remove_url_duplicates(vehicle_type)\n",
    "    counter += new_item_amount\n",
    "\n",
    "print(f'\\n\\tTotal amount of URLs added: {counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oryx Scrape\n",
    "print('Scraping Oryx:')\n",
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    new_item_amount, total_amount = oryx_scrape(vehicle_type)\n",
    "    # print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "    # remove_url_duplicates(vehicle_type)\n",
    "\n",
    "# oryx_scrape('T-62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warspotting Scrape (async)\n",
    "import concurrent.futures\n",
    "processes = []\n",
    "\n",
    "query_vehicle_list = [['T-90',1],['M113',2],['2S1',1],['2S3',2]] # Custom list for testing purposes\n",
    "\n",
    "print('Scraping Warspotting:')\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    if __name__ == \"__main__\":\n",
    "        for item in query_vehicle_list:\n",
    "            vehicle_type, _ = item\n",
    "            p = executor.submit(warspotting_scrape,vehicle_type)\n",
    "            processes.append([p,vehicle_type])\n",
    "        for p in processes:\n",
    "            process, vehicle_type = p\n",
    "            new_item_amount, total_amount = process.result()\n",
    "            print(f'Added {new_item_amount} media URLs of {vehicle_type}s to the list. Total number: {total_amount}')\n",
    "\n",
    "# Warspotting Scrape (sync)\n",
    "# warspotting_scrape('T-62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc19c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    remove_url_duplicates(vehicle_type)\n",
    "    #count_urls(vehicle_type)\n",
    "    #remove_all_but_twitter_urls(vehicle_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75333e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url_list = read_list('T-90')\n",
    "df = pd.DataFrame(url_list, columns = ['URL', 'Status', 'Media Type', 'Source'])\n",
    "print(df['Source'].value_counts())\n",
    "print('===')\n",
    "print(df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Function downloads images from URLs and assigns them the original name from the same URL. It's done this way in order to prevent downloading the same image twice.\n",
    "def download_img(url, vehicle_type):\n",
    "    if url is not None:\n",
    "        # url received = [url, status, media_type, source]\n",
    "        url, status, media_type, source = url\n",
    "        if media_type == 'video':   \n",
    "            return 'Video'    \n",
    "        elif media_type == 'image':   # if 'https://twitter.com/' not in url:\n",
    "            file_name_temp = url.split('/')\n",
    "            if source == 'twitter':\n",
    "                file_name = file_name_temp[-1]\n",
    "            else: # If source is oryx then we concat the two parts of the URL, since the last one is not unique. \n",
    "                file_name = file_name_temp[-2] + file_name_temp[-1]\n",
    "            full_path = os.path.join('./dataset/',vehicle_type,status,source,file_name) \n",
    "            if not os.path.exists(full_path):\n",
    "                os.makedirs(os.path.dirname(full_path), exist_ok=True) # Creates folder if it doesn't exists previously\n",
    "                r = requests.get(url)  \n",
    "                with open(full_path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return 'Downloaded'\n",
    "            else:\n",
    "                # Picture already exists\n",
    "                return 'Exists'\n",
    "        elif media_type == 'website':\n",
    "            pass\n",
    "        else: # media_type == 'unknown'\n",
    "            pass\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vehicle_list2 = [\n",
    "    ['M113',1],\n",
    "    ['T-64',2]\n",
    "]\n",
    "for item in query_vehicle_list2:\n",
    "    counter_downloaded = 0\n",
    "    counter_pic_exists = 0\n",
    "    counter_tw_videos = 0\n",
    "\n",
    "    vehicle_type, _ = item\n",
    "    remove_url_duplicates(vehicle_type)\n",
    "    url_list = read_list(vehicle_type)\n",
    "    print('+ Out of', len(url_list), vehicle_type, 'URLs:')\n",
    "    for url in url_list:\n",
    "        outcome = download_img(url, vehicle_type)\n",
    "        if outcome == 'Downloaded': # Downloaded successfully\n",
    "            counter_downloaded += 1\n",
    "        elif outcome == 'Exists':   # Picture already exists\n",
    "            counter_pic_exists += 1\n",
    "        else: # outcome == 'Video': - Link belongs to a Twitter video\n",
    "            counter_tw_videos += 1\n",
    "\n",
    "    print('   -', counter_downloaded, 'new pictures were downloaded')\n",
    "    print('   -', counter_pic_exists, 'pictures already existed')\n",
    "    print('   -', counter_tw_videos, 'URLs belonged to Twitter videos')\n",
    "\n",
    "#download_img('https://twitter.com/UAWeapons/status/1564984095591088129','2S3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import imagehash\n",
    "import os\n",
    "import numpy as np\n",
    "IMAGE_EXTENSIONS = ['.jpg','.jpeg','.bmp','.png', '.gif', '.tiff']\n",
    "\n",
    "def find_duplicates(folder_path,delete_duplicates,verbose=False):\n",
    "        hash_size = 8\n",
    "        \n",
    "        fnames = os.listdir(folder_path)\n",
    "        hashes = {}\n",
    "        duplicates = []\n",
    "\n",
    "        for image in fnames:\n",
    "            if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "                if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                    with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                        temp_hash = imagehash.average_hash(img, hash_size)\n",
    "                        if temp_hash in hashes:\n",
    "                            # print('Duplicate {} \\nfound for Image {}!\\n'.format(image,hashes[temp_hash]))\n",
    "                            if image not in duplicates:\n",
    "                                duplicates.append(image)\n",
    "                        else:\n",
    "                            hashes[temp_hash] = image\n",
    "        \n",
    "        vehicle_name = folder_path.split('/')[-1]\n",
    "        if verbose:\n",
    "            print('\\t',vehicle_name,'- Duplicates:')\n",
    "\n",
    "        if len(duplicates) != 0:\n",
    "            if delete_duplicates:\n",
    "                for img in duplicates:\n",
    "                    os.remove(os.path.join(folder_path,img))\n",
    "                if verbose:\n",
    "                    print('\\t\\t',len(duplicates),'deleted')\n",
    "            else:\n",
    "                if verbose:\n",
    "                    for img in duplicates:\n",
    "                        print('\\t\\t',img)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('\\t\\tNo duplicates found') \n",
    "\n",
    "        return len(duplicates) or 0\n",
    "\n",
    "def find_similar(image_path,folder_path,delete_duplicates,similarity,verbose=False):\n",
    "    hash_size = 8\n",
    "\n",
    "    fnames = os.listdir(folder_path)\n",
    "    threshold = 1 - similarity/100\n",
    "    diff_limit = int(threshold*(hash_size**2))\n",
    "\n",
    "    duplicates = []\n",
    "    image_name = ''\n",
    "    \n",
    "    if os.path.getsize(image_path) > 0:\n",
    "        with Image.open(image_path) as img:\n",
    "            hash1 = imagehash.average_hash(img, hash_size).hash\n",
    "    \n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            if os.path.getsize(os.path.join(folder_path,image)) > 0:\n",
    "                with Image.open(os.path.join(folder_path,image)) as img:\n",
    "                    hash2 = imagehash.average_hash(img, hash_size).hash\n",
    "                    if np.count_nonzero(hash1 != hash2) <= diff_limit:\n",
    "                        image_name_temp = image_path.split('/')\n",
    "                        image_name = image_name_temp[-1]\n",
    "                        if image_name != image: # Makes sure the original picture is not added to the list of duplicates\n",
    "                            duplicates.append(image)\n",
    "                            # print('{} image found {}% similar to {}'.format(image,similarity,folder_path))\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\tOriginal:')\n",
    "        print('\\t\\t',image_name)\n",
    "        print('\\tDuplicates:')\n",
    "\n",
    "    if len(duplicates) != 0:\n",
    "        if delete_duplicates:\n",
    "            for img in duplicates:\n",
    "                os.remove(os.path.join(folder_path,img))\n",
    "            if verbose:\n",
    "                print('\\t\\t',len(duplicates))\n",
    "        else:\n",
    "            if verbose:\n",
    "                for img in duplicates:\n",
    "                    print('\\t\\t',img)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('\\t\\tNo close duplicates found') \n",
    "    \n",
    "    return len(duplicates) or 0\n",
    "    \n",
    "#find_similar('./dd/2S1 - Copy/R0r6NcvQ5577.png','./dd/2S1 - Copy/',False,97,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_duplicates('./dd/orig1/',True)\n",
    "\n",
    "#for item in query_vehicle_list:\n",
    "\n",
    "folder_path = './dd/c3/'\n",
    "fnames = os.listdir(folder_path)\n",
    "for image in fnames:\n",
    "    image_path = os.path.join(folder_path,image)\n",
    "    if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "        find_similar(image_path,folder_path,True,97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6e9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in query_vehicle_list:\n",
    "    vehicle_type, _ = item\n",
    "    folder_path = './dataset/' + vehicle_type\n",
    "    amount_duplicates = find_duplicates(folder_path, True)\n",
    "    \n",
    "    total_close_duplicates = 0\n",
    "    folder_path += '/'\n",
    "    fnames = os.listdir(folder_path)\n",
    "    for image in fnames:\n",
    "        if any(x in image for x in IMAGE_EXTENSIONS):\n",
    "            image_path = os.path.join(folder_path,image)\n",
    "            if Path(image_path).is_file(): # Checks if the file exists, since it could've been deleted inside the function\n",
    "                amount_close_duplicates = find_similar(image_path,folder_path,True,97)\n",
    "                total_close_duplicates += amount_close_duplicates\n",
    "    print(vehicle_type, '-', amount_duplicates, 'duplicates and', total_close_duplicates, 'close duplicates deleted.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79ae438818e454af4f26ac53bc90cc6114af98306db0d7e90db8f6b9a9700471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
